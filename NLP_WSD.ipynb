{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mansoor Nabawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP final summer 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nabaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nabaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nabaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "from glob import glob\n",
    "import string\n",
    "import pandas as pd\n",
    "# from pandarallel import pandarallel #for parallel processing.\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# from numba import jit\n",
    "# import numba\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import swifter #parallel computation for pandas\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import html\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. converting semcor files to xml in order to read them and do further processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to the files\n",
    "#path = 'c:\\\\Users\\\\nabaw\\\\Desktop\\\\NLP\\\\semcor1.7.1'\n",
    "# add quotes around XML attributes, rename files with \".xml\" extension\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "# #going through all the files and convert them to xml.\n",
    "for f in glob(\"semcor1.7.1/semcor1.7.1/brown*/tagfiles/*\"):\n",
    "    c = open(f).read()\n",
    "    c = re.sub(r'&', r'&amp;', c)\n",
    "    for i in range(10):\n",
    "        c = re.sub(r'(<[^>]+=)([^\">]+)([ >])', r'\\1\"\\2\"\\3', c)\n",
    "    f2 = open(f + \".xml\", \"w\")\n",
    "    f2.write(c)\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in order to read the datasets we need use xml libraries and we also need to keep track of attributes of words as they are very important.\n",
    "- Therefore, we create a class called word where we can save every attribute of a word in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Word:\n",
    "    \"\"\"a class to store words attribute.\"\"\"\n",
    "    def __init__(self, text, pos=None, lemma=None, wnsn=None, lexsn=None, id=None):\n",
    "        self.text = text\n",
    "        self.pos = pos\n",
    "        self.lemma = lemma\n",
    "        self.wnsn = wnsn\n",
    "        self.lexsn = lexsn\n",
    "        self.id = id #for senseval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semcor dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- after converting files to xml we start reading all the files in all the folders.\n",
    "- we use glob to go through all the subdirectories and read the semcor files.\n",
    "- we xml library to parse the files and go through tags and read words and its attributes in each sentences.\n",
    "- we create a list called documents to keep the sentences in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 37176 number of documents \n"
     ]
    }
   ],
   "source": [
    "# let us read xml file\n",
    "documents = []\n",
    "for file in glob(\"semcor1.7.1/semcor1.7.1/brown*/tagfiles/*.xml\"):\n",
    "    tree = ET.parse(file)\n",
    "\n",
    "    # get the root element\n",
    "    root = tree.getroot()\n",
    "    # let us read every sentence, one-by-one\n",
    "    # we are ignoring the paragraph structure\n",
    "    for sentence_tree in root.findall('context/p/'):\n",
    "        sentence = []\n",
    "    #     for every word in that sentence\n",
    "        for word_tree in sentence_tree:\n",
    "    #         get the word\n",
    "            word = Word(word_tree.text)\n",
    "            \n",
    "    #         if the word xml tag contains info about pos, lemma, wnsn, lexsn, then extract it\n",
    "            if 'pos' in word_tree.attrib:\n",
    "                word.pos = word_tree.attrib['pos']\n",
    "                \n",
    "            if 'lemma' in word_tree.attrib:\n",
    "                word.lemma = word_tree.attrib['lemma']\n",
    "                \n",
    "            if 'wnsn' in word_tree.attrib:\n",
    "                word.wnsn = word_tree.attrib['wnsn']\n",
    "                \n",
    "            if 'lexsn' in word_tree.attrib:\n",
    "                word.lexsn = word_tree.attrib['lexsn']\n",
    "            \n",
    "            sentence.append( word )\n",
    "        documents.append(sentence)\n",
    "\n",
    "print('Read {0} number of documents '.format(len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can have a look at one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The jury further said in term end presentments that the City_Executive_Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City_of_Atlanta '' for the manner in which the election was conducted .\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([x.text for x in documents[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jury%1:14:01::', 'panel%1:14:01::']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l.key() for l in wn.synsets(\"jury\")[1].lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', None, None],\n",
       " ['jury', 'jury', '1:14:00::'],\n",
       " ['further', 'far', '4:02:00::'],\n",
       " ['said', 'say', '2:32:00::'],\n",
       " ['in', None, None],\n",
       " ['term', 'term', '1:28:00::'],\n",
       " ['end', 'end', '1:28:00::'],\n",
       " ['presentments', 'presentment', '1:04:00::'],\n",
       " ['that', None, None],\n",
       " ['the', None, None],\n",
       " ['City_Executive_Committee', 'group', '1:03:00::'],\n",
       " [',', None, None],\n",
       " ['which', None, None],\n",
       " ['had', 'have', '2:40:04::'],\n",
       " ['over-all', 'overall', '5:00:00:gross:00'],\n",
       " ['charge', 'charge', '1:04:03::'],\n",
       " ['of', None, None],\n",
       " ['the', None, None],\n",
       " ['election', 'election', '1:04:01::'],\n",
       " [',', None, None],\n",
       " ['``', None, None],\n",
       " ['deserves', 'deserve', '2:42:00::'],\n",
       " ['the', None, None],\n",
       " ['praise', 'praise', '1:10:00::'],\n",
       " ['and', None, None],\n",
       " ['thanks', 'thanks', '1:10:00::'],\n",
       " ['of', None, None],\n",
       " ['the', None, None],\n",
       " ['City_of_Atlanta', 'location', '1:03:00::'],\n",
       " [\"''\", None, None],\n",
       " ['for', None, None],\n",
       " ['the', None, None],\n",
       " ['manner', 'manner', '1:07:02::'],\n",
       " ['in', None, None],\n",
       " ['which', None, None],\n",
       " ['the', None, None],\n",
       " ['election', 'election', '1:04:01::'],\n",
       " ['was', None, None],\n",
       " ['conducted', 'conduct', '2:41:00::'],\n",
       " ['.', None, None]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[x.text, x.lemma, x.lexsn] for x in documents[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can select 5000 sentences randomly as it is asked by the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabaw\\AppData\\Local\\Temp\\ipykernel_23188\\3888831070.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample_semcor = np.random.choice( documents, size=5000)\n"
     ]
    }
   ],
   "source": [
    "#fix seed to produce same result everytime\n",
    "np.random.seed(2022)\n",
    "sample_semcor = np.random.choice( documents, size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"On some of the islands ' beaches the waves came_in gently ; they were steepest on the shores facing the direction of the seaquake from which the waves had come .\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([x.text for x in sample_semcor[10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Senseval2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- everything the same as we read the Semcor dataset, the only difference is we are using lxml dataset as the structure of the Sensevals are a bit different.\n",
    "- we keep the dataset in the doc_senseval2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 238 number of doc_senseval2 \n"
     ]
    }
   ],
   "source": [
    "#senseval2\n",
    "# let us read xml file\n",
    "#from lxml import html\n",
    "doc_senseval2 = []\n",
    "for file in glob(\"senseval2.semcor/senseval2.semcor/wordnet1.7.1/*\"):\n",
    "\n",
    "    tree = html.fromstring(open(file).read())\n",
    "    #tree = ET.parse(open(file))\n",
    "\n",
    "    # get the root element\n",
    "    #root = tree.getroot()\n",
    "    # let us read every sentence, one-by-one\n",
    "    for sentence_tree in tree.getchildren():\n",
    "        sentence = []\n",
    "        #     for every word in that sentence\n",
    "        for word_tree in sentence_tree:\n",
    "    #         get the word\n",
    "            word = Word(word_tree.text)\n",
    "            #print(word)\n",
    "            \n",
    "    #         if the word xml tag contains info about pos, lemma, wnsn, lexsn, then extract it\n",
    "            if 'pos' in word_tree.attrib:\n",
    "                word.pos = word_tree.attrib['pos']\n",
    "                \n",
    "            if 'lemma' in word_tree.attrib:\n",
    "                word.lemma = word_tree.attrib['lemma']\n",
    "                \n",
    "            if 'wnsn' in word_tree.attrib:\n",
    "                word.wnsn = word_tree.attrib['wnsn']\n",
    "                \n",
    "            if 'lexsn' in word_tree.attrib:\n",
    "                word.lexsn = word_tree.attrib['lexsn']\n",
    "            \n",
    "            if 'id' in word_tree.attrib:\n",
    "                word.id = word_tree.attrib['id']\n",
    "            \n",
    "            sentence.append( word )\n",
    "        doc_senseval2.append(sentence)\n",
    "\n",
    "print('Read {0} number of doc_senseval2 '.format(len(doc_senseval2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" We \\'ve tried to train the youngsters , but they have their discos and their dances , and they just drift away .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#an example\n",
    "\n",
    "\" \".join([x.text for x in doc_senseval2[10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Senseval3 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- same as senseval2.\n",
    "- the data is saved in doc_senseval3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 300 number of doc_senseval3 \n"
     ]
    }
   ],
   "source": [
    "#senseval3\n",
    "\n",
    "# let us read xml file\n",
    "#from lxml import html\n",
    "doc_senseval3 = []\n",
    "for file in glob(\"senseval3.semcor/senseval3.semcor/wordnet1.7.1/*\"):\n",
    "\n",
    "    tree = html.fromstring(open(file).read())\n",
    "    #tree = ET.parse(open(file))\n",
    "\n",
    "    # get the root element\n",
    "    #root = tree.getroot()\n",
    "    # let us read every sentence, one-by-one\n",
    "    # we are ignoring the paragraph structure\n",
    "    for sentence_tree in tree.getchildren():\n",
    "        sentence = []\n",
    "        #     for every word in that sentence\n",
    "        for word_tree in sentence_tree:\n",
    "    #         get the word\n",
    "            word = Word(word_tree.text)\n",
    "            #print(word)\n",
    "            \n",
    "    #         if the word xml tag contains info about pos, lemma, wnsn, lexsn, then extract it\n",
    "            if 'pos' in word_tree.attrib:\n",
    "                word.pos = word_tree.attrib['pos']\n",
    "                \n",
    "            if 'lemma' in word_tree.attrib:\n",
    "                word.lemma = word_tree.attrib['lemma']\n",
    "                \n",
    "            if 'wnsn' in word_tree.attrib:\n",
    "                word.wnsn = word_tree.attrib['wnsn']\n",
    "                \n",
    "            if 'lexsn' in word_tree.attrib:\n",
    "                word.lexsn = word_tree.attrib['lexsn']\n",
    "            \n",
    "            if 'id' in word_tree.attrib:\n",
    "                word.id = word_tree.attrib['id']\n",
    "            \n",
    "            sentence.append( word )\n",
    "        doc_senseval3.append(sentence)\n",
    "\n",
    "print('Read {0} number of doc_senseval3 '.format(len(doc_senseval3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Brakes howled and a horn blared furiously , but the man would have been hit if Phil had n't called_out to him a second before .\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#an example\n",
    "\n",
    "\" \".join([x.text for x in doc_senseval3[10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will read the embeddings on numpy/pandas arrasy/dataframes.\n",
    "- we read the files.\n",
    "- we split the data by lines \"\\n\"\n",
    "- again we split by space to create an id and a word%lexsn array.\n",
    "- we keep them in a list.\n",
    "- finally make a Pandas DataFrame out of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to data\n",
    "lexemes_path = \"embeddings/lexemes.txt\"\n",
    "mapping_path = \"embeddings/mapping.txt\"\n",
    "sysnsets_path = \"embeddings/synsets.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mapping_path) as f:\n",
    "    doc= f.read()\n",
    "\n",
    "doc = doc.split(\"\\n\")\n",
    "mapp_df = []\n",
    "for d in doc:\n",
    "    mapp_df.append(d.split())\n",
    "mapp_df = pd.DataFrame(mapp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wn-2.1-00001742-n</td>\n",
       "      <td>entity%1:03:00::,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wn-2.1-00002219-n</td>\n",
       "      <td>thing%1:03:00::,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wn-2.1-00002361-n</td>\n",
       "      <td>anything%1:03:00::,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wn-2.1-00002471-n</td>\n",
       "      <td>something%1:03:00::,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wn-2.1-00002579-n</td>\n",
       "      <td>nothing%1:03:00::,nonentity%1:03:00::,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                                       1\n",
       "0  wn-2.1-00001742-n                       entity%1:03:00::,\n",
       "1  wn-2.1-00002219-n                        thing%1:03:00::,\n",
       "2  wn-2.1-00002361-n                     anything%1:03:00::,\n",
       "3  wn-2.1-00002471-n                    something%1:03:00::,\n",
       "4  wn-2.1-00002579-n  nothing%1:03:00::,nonentity%1:03:00::,"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at the first 5 rows\n",
    "mapp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexemes/Synsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KeyedVectors from gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#loading embeddings\n",
    "lexemes_emb = KeyedVectors.load_word2vec_format(lexemes_path, binary=False)\n",
    "synsets_emb = KeyedVectors.load_word2vec_format(sysnsets_path, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common Sense and Lesk Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "#get accuracy\n",
    "def get_acc(res):\n",
    "    \"\"\"a function to show accuracy by finding the intersection between two sets.\"\"\"\n",
    "    all = 0\n",
    "    corr = 0\n",
    "    for r in res:\n",
    "        for word in res[r]:\n",
    "            #print(word)\n",
    "            lemmas = set([l.key().lower() for l in word[2].lemmas()])\n",
    "            lem = set(word[1].lower().split(\",\"))\n",
    "            if \"none\" not in word[1].split(\"%\"):\n",
    "                all += 1\n",
    "            #if there are common elements between true lemmas and all lemmas in the sysnet\n",
    "                if len(lem.intersection(lemmas))>0:\n",
    "                    corr += 1\n",
    "    return (corr/all)*100, corr, all\n",
    "#making lemma\n",
    "def lemma_mixer(lem, lex):\n",
    "    \"\"\"a function to create lemma keys out of lemma and lexsn\"\"\"\n",
    "    lem = str(lem).lower()\n",
    "    lex = str(lex).lower().split(\";\")\n",
    "    for i in range(len(lex)):\n",
    "        lex[i]= lem+\"%\"+lex[i]\n",
    "    return \",\".join(lex)\n",
    "\n",
    "#a function to find the most common sense for each word\n",
    "def find_most_common_sense(docs):\n",
    "    #to store the data\n",
    "    full_sent = {}\n",
    "    #going through every sentence\n",
    "    for doc in docs:\n",
    "        #storing pos, text and lemmakey for each word in a list\n",
    "        pos_text = [[x.pos, x.text, lemma_mixer(x.lemma, x.lexsn)] for x in doc]\n",
    "        #key for our dictionary\n",
    "        kk = \" \".join([x.text for x in doc])\n",
    "        #an empty list for words\n",
    "        full_sent[kk] = []\n",
    "        #going through every word and its attributes.\n",
    "        for pt in pos_text:\n",
    "            pos = pt[0]\n",
    "            #clean the text\n",
    "            text = utils.simple_preprocess(pt[1])\n",
    "            #if the pos exists search for the most common sense with that pos\n",
    "            if pos !=None:\n",
    "                try:\n",
    "                #getting the very first\n",
    "                    pos = pos[0].lower()\n",
    "                    syns = wn.synsets(\" \".join(text), pos)[0]\n",
    "                except:\n",
    "                    continue\n",
    "                    #syns = None\n",
    "            #if there is no pos get the very first synset\n",
    "            else:\n",
    "                try:\n",
    "                    syns = wn.synsets(text)[0]\n",
    "                except:\n",
    "                    continue\n",
    "                    #syns = None\n",
    "            #save the result in that sentece with this order: text, true lemma, synset\n",
    "            full_sent[kk].append([text,pt[2], syns])\n",
    "    \n",
    "    return full_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.86358866736621, 1141, 1906)"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms2 = find_most_common_sense(doc_senseval2)\n",
    "get_acc(ms2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['art'], 'art%1:09:00::', Synset('art.n.01')],\n",
       " [['is'], 'be%2:42:03::', Synset('be.v.01')],\n",
       " [['english'], 'english%1:18:00::', Synset('english.n.01')],\n",
       " [['peculiarities'],\n",
       "  'peculiarity%1:09:00::,peculiarity%1:07:02::',\n",
       "  Synset('peculiarity.n.01')],\n",
       " [['rest'], 'rest%1:24:00::', Synset('remainder.n.01')],\n",
       " [['world'], 'world%1:14:02::', Synset('universe.n.01')]]"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms2[list(ms2)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.813084112149525, 960, 1605)"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms3 = find_most_common_sense(doc_senseval3)\n",
    "get_acc(ms3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66.76295593688023, 15485, 23194)"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msc = find_most_common_sense(sample_semcor)\n",
    "get_acc(msc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['houghton'], 'none%none', Synset('houghton.n.01')],\n",
       " [['said'], 'say%2:32:00::', Synset('state.v.01')],\n",
       " [['had'], 'none%none', Synset('have.v.01')],\n",
       " [['been'], 'none%none', Synset('be.v.01')],\n",
       " [['set'], 'set%2:32:00::', Synset('put.v.01')],\n",
       " [['canvass'], 'none%none', Synset('canvas.n.03')],\n",
       " [['homes'], 'none%none', Synset('home.n.01')],\n",
       " [['subdivision'], 'none%none', Synset('subdivision.n.01')],\n",
       " [['is'], 'none%none', Synset('be.v.01')],\n",
       " [['located'], 'locate%2:42:00::', Synset('locate.v.01')],\n",
       " [['northeast'], 'none%none', Synset('northeast.n.01')],\n",
       " [['mile'], 'none%none', Synset('mile.n.01')],\n",
       " [['road'], 'none%none', Synset('road.n.01')]]"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msc[list(msc)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we go through every sentence.\n",
    "- we collect word text for every sentence.\n",
    "- we exculde digits, stop_words and punctuations.\n",
    "- for every word in the sentences:\n",
    "    - we find the synsets using wordnet.\n",
    "    - for every synset:\n",
    "        - gather definition.\n",
    "        - gather examples.\n",
    "        - we keep both in two different dictionaries the word is the key and the values are sense and its defintion/examples.\n",
    "    - we go through every word in the definition dictionary.\n",
    "        -  and find the overlaps between the context and definition as well as examples.\n",
    "    - the sense with the most overlps/most common words is chosen.\n",
    "    - if the overlap is 0, then choose the very first one.\n",
    "- keep all the data in one dictionary.\n",
    "    - dictionary key is the full sentence.\n",
    "    - values are the words with their senses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(sample):\n",
    "    \"\"\"a function get accuracy. by finding the intersection between two sets\"\"\"\n",
    "    all = 0\n",
    "    corr = 0\n",
    "    for s in sample:\n",
    "        for word in sample[s]:\n",
    "            true_ = word[1]\n",
    "            all_lemmas = word[3]\n",
    "            if \"none\" not in true_.split(\"%\"):\n",
    "                all += 1\n",
    "                if len(set(true_.split(\",\")).intersection(set([x.lower() for x in all_lemmas])))>0:\n",
    "                    corr += 1\n",
    "\n",
    "    return (corr/all)*100, corr, all\n",
    "\n",
    "\n",
    "def lesk_mc(sample_sentences):\n",
    "    \"\"\"Plain lesk algorithm\"\"\"\n",
    "    puncs = set(string.punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    full_sent_dic = {}\n",
    "\n",
    "    #going through every sentence.\n",
    "    for sample in tqdm(sample_sentences):\n",
    "        #full sentence\n",
    "        sentence = [[s.text, lemma_mixer(s.lemma, s.lexsn)] for s in sample] \n",
    "        #making a lemma key for each word\n",
    "        #filtering sentence, no stopwords included and no \",\" , \".\"\n",
    "        filtered_sentence = [word for word in sentence if word[0] not in stop_words]\n",
    "        #not considering numbers \n",
    "        filtered_sentence = [x for x in filtered_sentence if not x[0].isdigit()]\n",
    "        #removing punctuations\n",
    "        filtered_sentence = [x for x in filtered_sentence if x[0] not in puncs]\n",
    "        #a dictionary to keep definitions and examples of each words.\n",
    "        sens_def_dic = {}\n",
    "        # going trough each word in the filtered sentence\n",
    "        for w in filtered_sentence:\n",
    "            word_ = w[0]\n",
    "            true_lemma = w[1]\n",
    "            #find the senses for a specific w (word)\n",
    "            senses = wn.synsets(word_)\n",
    "            #making a list for each word to contain the definitions and examples.\n",
    "            sens_def_dic[word_] = []\n",
    "            for sens in senses:\n",
    "                # appending all the senses and examples and true lemmas in the dictionaries.\n",
    "                sens_def_dic[word_].append([sens, sens.definition(), sens.examples(), true_lemma])\n",
    "\n",
    "        #going through each words definitions\n",
    "        kkey = \" \".join([s[0] for s in sentence])\n",
    "        full_sent_dic[kkey] = []\n",
    "        #just keeping the text\n",
    "        context = [s[0] for s in filtered_sentence]\n",
    "        for w in sens_def_dic:\n",
    "            #print(filtered_sentence[0])\n",
    "            #print(w)\n",
    "            max = 0\n",
    "            index = 0\n",
    "            #every definition of each word\n",
    "            for df in sens_def_dic[w]:\n",
    "                #keeping the counts of overlaps\n",
    "                cnt = 0\n",
    "                #-------------------\n",
    "                #overlaps between the sentence and the definion\n",
    "                def_ = [w.lower() for w in df[1] if w not in stop_words]\n",
    "                examples_ = \" \".join(df[2]).split()\n",
    "                examples_ = [exm.lower() for exm in examples_ if exm not in stop_words]\n",
    "                #---------------------\n",
    "                common = set(examples_).intersection(def_)\n",
    "                common = set(context).intersection(common)\n",
    "\n",
    "                #count of overlaps\n",
    "                cnt += len(common)\n",
    "                #if there is overlap and it is bigger than the max then this is selected as our sens\n",
    "                if max < cnt:\n",
    "                    max = cnt\n",
    "                    index = df\n",
    "                else:\n",
    "                    #if no overlapping found, choose the very first definition.\n",
    "                    index = sens_def_dic[w][0]\n",
    "            #print(w + \" : \" + str(index))\n",
    "            #excluding words with no meaning\n",
    "            if index != 0:\n",
    "                full_sent_dic[kkey].append([w,index[-1],index[0], [l.key() for l in index[0].lemmas()]])\n",
    "            #i += 1\n",
    "    return full_sent_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lesk for Semcor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:11<00:00, 446.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53.411108293177776, 12636, 23658)"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor = lesk_mc(sample_semcor)\n",
    "evaluate_accuracy(semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said',\n",
       " 'say%2:32:00::',\n",
       " Synset('state.v.01'),\n",
       " ['state%2:32:00::', 'say%2:32:00::', 'tell%2:32:04::']]"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor[list(semcor.keys())[0]][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lesk for Sensevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:00<00:00, 362.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48.87459807073955, 1064, 2177)"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senseval2 = lesk_mc(doc_senseval2)\n",
    "evaluate_accuracy(senseval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 448.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49.11072862880092, 856, 1743)"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senseval3_lesk = lesk_mc(doc_senseval3)\n",
    "evaluate_accuracy(senseval3_lesk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oele and van Noord (2017) method using the pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have embeddings for lexemes and sysnsets.\n",
    "- we need to compute the embeddings for the sentences we have.\n",
    "- therefore we use Word2vec to produce the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping NaN values\n",
    "mapp_df= mapp_df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim import utils\n",
    "#loading embeddings\n",
    "word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapp_df.columns = [\"wkey\", \"wordlemma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapp_df[\"keyz\"] = mapp_df.apply(lambda x: lex_creator(x.wkey, x.wordlemma), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from operator import itemgetter\n",
    "from gensim import matutils\n",
    "\n",
    "def get_acc(true_senses, result):\n",
    "    \"\"\"finding accuracy.\"\"\"\n",
    "    all_lems = []\n",
    "    for s in true_senses:\n",
    "        for l in s.lemmas():\n",
    "            all_lems.append(l.key())\n",
    "    try:\n",
    "        res = result[:-1][0]\n",
    "        f = []\n",
    "        for r in res:\n",
    "            for k in r.split(\",\"):\n",
    "                if len(k)>0:\n",
    "                    f.append(k)\n",
    "        #res = set([res.lower() for res in res.split(\",\") if len(res)>0])\n",
    "        res = set(f)\n",
    "        all_lems = set(s.lower() for s in all_lems)\n",
    "        if len(all_lems.intersection(res))>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def show_acc(df):\n",
    "    \"\"\"a function to show final accuracy.\"\"\"\n",
    "    res = df.apply(lambda x: get_acc(x.true_synset, x.result), axis=1).dropna()\n",
    "    corr = res.sum()\n",
    "    all = res.shape[0]\n",
    "\n",
    "    return (corr/all)*100, corr, all\n",
    "    \n",
    "def similarity_cosine(vec1, vec2):\n",
    "    \"\"\"a function to compute cosine similarity between two vectors.\"\"\"\n",
    "    cosine_similarity = np.dot(matutils.unitvec(vec1.reshape(1,-1)), matutils.unitvec(vec2))\n",
    "    return cosine_similarity\n",
    "\n",
    "def lemma_mixer(lem, lex):\n",
    "    \"\"\"a function to create lemma keys out of lemma and lexsn.\"\"\"\n",
    "    lem = str(lem).lower()\n",
    "    lex = str(lex).lower().split(\";\")\n",
    "    for i in range(len(lex)):\n",
    "        lex[i]= lem+\"%\"+lex[i]\n",
    "    return \",\".join(lex)\n",
    "\n",
    "def gloss_vec(x, model):\n",
    "    \"\"\"a function to create gloss vector for a given word using a word2vec model.\"\"\"\n",
    "    defs_ = []\n",
    "    for syn in x:\n",
    "        sc = utils.simple_preprocess(syn.definition())\n",
    "        sc = [token for token in sc if token in model.index_to_key and token not in stopwords.words('english')]\n",
    "        try:\n",
    "            sc = np.mean(model[sc], axis=0)\n",
    "        except:\n",
    "            sc = np.zeros((1,300), dtype=float)\n",
    "        defs_.append(sc)\n",
    "    return defs_\n",
    "    \n",
    "def example_vec(x, model):\n",
    "    \"\"\"a function to create example vector for a give word using word2vec model.\"\"\"\n",
    "    defs_ = []\n",
    "    for syn in x:\n",
    "        sc = utils.simple_preprocess(\".\".join(syn.examples()))\n",
    "        sc = [token for token in sc if token in model.index_to_key and token not in stopwords.words('english')]\n",
    "        try:\n",
    "            sc = np.mean(model[sc], axis=0)\n",
    "        except:\n",
    "            sc = np.zeros((1,300), dtype=float)\n",
    "        defs_.append(sc)\n",
    "    return defs_\n",
    "\n",
    "#belows are some helper functions.\n",
    "def lemma_finder(wn_syn, lem_n):\n",
    "    lem_n = lem_n[0].split(\"%\")[0]+\"%\"\n",
    "    lem_syns = []\n",
    "    for lem in wn_syn:\n",
    "        lems = []\n",
    "        for lem in lem.lemmas():\n",
    "            if str(lem.key()).startswith(lem_n):\n",
    "                lems.append(lem.key())\n",
    "        if len(lems)>0:\n",
    "            lem_syns.append(lems)\n",
    "    return lem_syns\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "def finder_l(x, w):\n",
    "    xx = set(wx for wx in x.lower().split(\",\") if len(wx)>0)\n",
    "    w = [y[0] for y in w]\n",
    "    ww = set(x.lower() for x in w if len(x)>0)\n",
    "    #looking for lexemes with lemma key.\n",
    "    res = bool(xx & ww)\n",
    "    if res:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def mapp_finder(df, w):\n",
    "    return df[df[\"wordlemma\"].apply(lambda x: finder_l(x, w))]\n",
    "\n",
    "def lex_creator(key, lemkey):\n",
    "    keyz = []\n",
    "    lemkey = [y for y in lemkey.split(\",\") if len(y)>0]\n",
    "    for x in lemkey:\n",
    "        if len(x)>0:\n",
    "            keyz.append(x.split(\"%\")[0]+\"-\"+key)\n",
    "    return keyz\n",
    "\n",
    "\n",
    "def disambiguator(word, true_lemmas, lemmas, gloss, examples, model, alpha, beta, lambda_):\n",
    "    \"\"\"the main function to perform disamiguation process.\"\"\"\n",
    "    if word not in context:\n",
    "        return None\n",
    "    #if no disambiguated words exist\n",
    "    if len(dis_vec)==0:\n",
    "        contxt_vec = model[context]\n",
    "    #if we had disamibuated words earlier \n",
    "    else:                      \n",
    "        alread_dis = np.concatenate(dis_vec, axis=0)\n",
    "        contxt_vec = np.concatenate((alread_dis,model[context]), axis=0)\n",
    "\n",
    "    context_mean_vec = np.mean(contxt_vec, axis=0)\n",
    "    #counter to catch the first word\n",
    "    i = 0\n",
    "    #highest cosine similarity\n",
    "    cos_high = 0\n",
    "    #best syns id\n",
    "    syns_id = 0\n",
    "    #syns of the chosen word\n",
    "    syns_word_selected = 0\n",
    "    #synset wordnet\n",
    "    synset_wd = 0\n",
    "    s = 0\n",
    "    #lem_n = true_lemmas[0].split(\"%\")[0]\n",
    "    keyz = mapp_finder(mapp_df, lemmas)\n",
    "    # keyz[\"lex_keyz\"] = keyz.apply(lambda x: lex_creator(x.wkey, x.wordlemma), axis=1)\n",
    "    #lemma key ---------------\n",
    "    for lem_syn in lemmas:\n",
    "        gloss_vec = gloss[s]\n",
    "        examples_vec = examples[s]\n",
    "        if (len(gloss_vec)>0 and alpha != 0):\n",
    "            score_sw = similarity_cosine(gloss_vec.flatten(), context_mean_vec.flatten()) #syns_embedding\n",
    "        else:\n",
    "            score_sw = 0\n",
    "        # keyz = mapp_finder(mapp_df, lem_syn)\n",
    "        if (len(examples_vec)>0 and lambda_ != 0):\n",
    "            score_example_s = similarity_cosine(examples_vec.flatten(), context_mean_vec.flatten()) \n",
    "        else:\n",
    "            score_example_s = 0\n",
    "\n",
    "        for syn in lem_syn:\n",
    "            #----------------\n",
    "            #exact map index\n",
    "            \n",
    "            map_idx = keyz[keyz.iloc[:,1].apply(lambda x: True if syn in x.lower().split(\",\") else False)]\n",
    "            #finding synset embedding\n",
    "            try:\n",
    "                map_lemmakey = map_idx.iloc[:,0].tolist()[0]\n",
    "            except:\n",
    "                continue\n",
    "            syns_embedding = synsets_emb[map_lemmakey]\n",
    "            #--------------------\n",
    "            if beta != 0:\n",
    "                #finding lexeme------------\n",
    "                lem_n = map_idx.iloc[:,1].tolist()[0].split(\"%\")[0]\n",
    "                lex_key = lem_n+\"-\"+map_lemmakey\n",
    "                try:\n",
    "                    lexeme_embedding = lexemes_emb[lex_key]\n",
    "                    score_lsw = similarity_cosine(lexeme_embedding.flatten(), context_mean_vec.flatten())\n",
    "                except:\n",
    "                    #continue\n",
    "                    score_lsw = 0#similarity_cosine(np.zeros((1,300), dtype=float).flatten(), context_mean_vec.flatten())\n",
    "            else:\n",
    "                score_lsw = 0\n",
    "            #---------------------------------\n",
    "\n",
    "            #final score\n",
    "            score = alpha*score_sw + beta*score_lsw + lambda_*score_example_s\n",
    "            if i == 0:\n",
    "                cos_high = score\n",
    "                syns_id = lem_syn#map_idx.iloc[:,1].tolist()[0]\n",
    "                syns_word_selected = syns_embedding#syns_embedding\n",
    "                #synset_wd = wn_syn\n",
    "                i += 1\n",
    "            if score>cos_high:\n",
    "                cos_high = score\n",
    "                syns_id = lem_syn#map_idx.iloc[:,1].tolist()[0]\n",
    "                syns_word_selected = syns_embedding#syns_embedding    \n",
    "                #synset_wd = wn_syn       \n",
    "        s += 1     \n",
    "\n",
    "    #if i was able to disambiguate the word add it so next time only the synset embedding is used\n",
    "    if i>0:\n",
    "        context.remove(word)\n",
    "        dis_vec.append(np.array(syns_word_selected, dtype=float).reshape(1,-1))\n",
    "    return [syns_id, cos_high]\n",
    "\n",
    "def final_wsd_(docs, model=word2vec, alpha=0.5, beta=0.1, lambda_=0.9):\n",
    "    \"\"\"Distirbutional Lesk algorithm, main function\"\"\"\n",
    "    full = []\n",
    "    for doc in tqdm(docs):\n",
    "    #one sentence\n",
    "        one_sent = utils.simple_preprocess(\" \".join([x.text for x in doc]))\n",
    "    #a list to store word, lemmakey, number of senses, sense key, syns\n",
    "        _sent = []\n",
    "    #every word in the sentence\n",
    "        for wrd in one_sent:\n",
    "        #make lemma key\n",
    "            lemk = [lemma_mixer(x.lemma, x.lexsn) for x in doc if ((\"\".join(utils.simple_preprocess(x.text)) == wrd)and((x.lexsn != None) and (x.lemma != None)))]\n",
    "        # if there is lexsn and lemma\n",
    "        #if len(lemk)>0:\n",
    "            try:\n",
    "            #making the lemma key, true one given by sentence.\n",
    "                lemk = \",\".join(lemk).split(\",\")\n",
    "                syns_t = [wn.lemma_from_key(l).synset() for l in lemk]\n",
    "            except:\n",
    "                continue\n",
    "            #lemk = None\n",
    "            #using wordnet, find all the synsets for the word\n",
    "            syns = wn.synsets(wrd)\n",
    "\n",
    "            _sent.append([wrd, lemk,syns_t, len(syns),  syns])\n",
    "        if len(_sent)==0:\n",
    "            continue\n",
    "        global dis_vec\n",
    "        dis_vec = []\n",
    "        #sorting\n",
    "        _sent = sorted(_sent, key=itemgetter(3))\n",
    "        global context\n",
    "        context = [token[0] for token in _sent if token[0] in model.index_to_key and token[0] not in stopwords.words('english')]\n",
    "        _sent = pd.DataFrame(_sent, columns=[\"word\", \"true_lemmas\",\"true_synset\", \"frequency\", \"all_synsets\"])\n",
    "        _sent[\"gloss\"] = _sent[\"all_synsets\"].apply(lambda x: gloss_vec(x, model))\n",
    "        _sent[\"examples\"] = _sent[\"all_synsets\"].apply(lambda x: example_vec(x, model))\n",
    "        _sent[\"all_lemmas\"]=_sent.apply(lambda x: lemma_finder(x.all_synsets, x.true_lemmas), axis=1)\n",
    "        _sent[\"result\"]=_sent.apply(lambda x: disambiguator(x.word, x.true_lemmas, x.all_lemmas, x.gloss, x.examples, model, alpha, beta, lambda_), axis=1)\n",
    "        full.append(_sent)\n",
    "    full = pd.concat(full)\n",
    "\n",
    "    return full.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [41:11<00:00,  8.24s/it] \n"
     ]
    }
   ],
   "source": [
    "#distributional lesk on senseval3\n",
    "a = final_wsd_(doc_senseval3, alpha=1, beta=1, lambda_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [43:50<00:00, 11.05s/it] \n"
     ]
    }
   ],
   "source": [
    "#distributional lesk on senseval2\n",
    "b = final_wsd_(doc_senseval2, alpha=1, beta=1, lambda_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributional lesk on sample semcor\n",
    "c = final_wsd_(sample_semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [52:08<00:00, 13.14s/it] \n"
     ]
    }
   ],
   "source": [
    "d = final_wsd_(doc_senseval2, alpha=0.5, beta=0.1, lambda_=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.59398125308337, 985.0, 2027)"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [51:48<00:00, 13.06s/it] \n"
     ]
    }
   ],
   "source": [
    "d1 = final_wsd_(doc_senseval2, alpha=0.8, beta=0, lambda_=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47.557967439565864, 964.0, 2027)"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [49:51<00:00, 12.57s/it] \n"
     ]
    }
   ],
   "source": [
    "d2 = final_wsd_(doc_senseval2, alpha=0, beta=0.8, lambda_=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45.831277750370006, 929.0, 2027)"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [45:38<00:00,  9.13s/it] \n"
     ]
    }
   ],
   "source": [
    "e1 = final_wsd_(doc_senseval3, alpha=0.5, beta=0.1, lambda_=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.40672782874618, 677.0, 1635)"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [43:51<00:00,  8.77s/it] \n"
     ]
    }
   ],
   "source": [
    "e2 = final_wsd_(doc_senseval3, alpha=0.8, beta=0, lambda_=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.813455657492355, 700.0, 1635)"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [44:12<00:00,  8.84s/it] \n"
     ]
    }
   ],
   "source": [
    "e3 = final_wsd_(doc_senseval3, alpha=0, beta=0.8, lambda_=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.391437308868504, 595.0, 1635)"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [12:08:28<00:00,  8.74s/it]  \n"
     ]
    }
   ],
   "source": [
    "c1 = final_wsd_(sample_semcor, alpha=0.5, beta=0.1, lambda_=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.52536852931093, 9457.0, 23336)"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [10:40:49<00:00,  7.69s/it]  \n"
     ]
    }
   ],
   "source": [
    "c2 = final_wsd_(sample_semcor, alpha=0.8, beta=0, lambda_=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>true_lemmas</th>\n",
       "      <th>true_synset</th>\n",
       "      <th>frequency</th>\n",
       "      <th>all_synsets</th>\n",
       "      <th>gloss</th>\n",
       "      <th>examples</th>\n",
       "      <th>all_lemmas</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>located</td>\n",
       "      <td>[locate%2:42:00::]</td>\n",
       "      <td>[Synset('situate.v.01')]</td>\n",
       "      <td>5</td>\n",
       "      <td>[Synset('locate.v.01'), Synset('situate.v.01')...</td>\n",
       "      <td>[[-0.072631836, 0.030517578, -0.040348597, 0.0...</td>\n",
       "      <td>[[0.02551651, 0.0703125, -0.01953125, 0.083292...</td>\n",
       "      <td>[[locate%2:40:00::], [locate%2:42:00::], [loca...</td>\n",
       "      <td>[[locate%2:42:00::], [0.6718176]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>said</td>\n",
       "      <td>[say%2:32:00::]</td>\n",
       "      <td>[Synset('state.v.01')]</td>\n",
       "      <td>12</td>\n",
       "      <td>[Synset('state.v.01'), Synset('allege.v.01'), ...</td>\n",
       "      <td>[[0.10583496, -0.028564453, 0.11706543, 0.1347...</td>\n",
       "      <td>[[0.024027506, -0.02319336, 0.025390625, 0.027...</td>\n",
       "      <td>[[say%2:32:00::], [say%2:32:01::], [say%2:32:0...</td>\n",
       "      <td>[[say%2:32:04::], [0.7423840704450704]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>set</td>\n",
       "      <td>[set%2:32:00::]</td>\n",
       "      <td>[Synset('specify.v.02')]</td>\n",
       "      <td>45</td>\n",
       "      <td>[Synset('set.n.01'), Synset('set.n.02'), Synse...</td>\n",
       "      <td>[[0.06587728, 0.04269409, 0.084360756, 0.18495...</td>\n",
       "      <td>[[0.026018415, 0.17170061, 0.113442555, 0.0655...</td>\n",
       "      <td>[[set%1:14:00::], [set%1:14:02::], [set%1:04:0...</td>\n",
       "      <td>[[set%5:00:00:arranged:00], [0.989714100853444]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>framework</td>\n",
       "      <td>[framework%1:09:00::]</td>\n",
       "      <td>[Synset('model.n.01')]</td>\n",
       "      <td>3</td>\n",
       "      <td>[Synset('model.n.01'), Synset('framework.n.02'...</td>\n",
       "      <td>[[0.07076416, -0.10839844, 0.08298111, 0.05541...</td>\n",
       "      <td>[[0.040754046, -0.007585798, 0.050432477, 0.09...</td>\n",
       "      <td>[[framework%1:09:00::], [framework%1:07:00::],...</td>\n",
       "      <td>[[framework%1:07:00::], [0.89532065]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apprehend</td>\n",
       "      <td>[apprehend%2:31:00::]</td>\n",
       "      <td>[Synset('grok.v.01')]</td>\n",
       "      <td>3</td>\n",
       "      <td>[Synset('grok.v.01'), Synset('collar.v.01'), S...</td>\n",
       "      <td>[[0.10872396, -0.075927734, 0.015950521, 0.103...</td>\n",
       "      <td>[[0.032552082, -0.050130207, 0.06640625, -0.08...</td>\n",
       "      <td>[[apprehend%2:31:00::], [apprehend%2:35:00::],...</td>\n",
       "      <td>[[apprehend%2:31:00::], [0.8523975690929537]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word            true_lemmas               true_synset  frequency  \\\n",
       "0    located     [locate%2:42:00::]  [Synset('situate.v.01')]          5   \n",
       "1       said        [say%2:32:00::]    [Synset('state.v.01')]         12   \n",
       "2        set        [set%2:32:00::]  [Synset('specify.v.02')]         45   \n",
       "3  framework  [framework%1:09:00::]    [Synset('model.n.01')]          3   \n",
       "4  apprehend  [apprehend%2:31:00::]     [Synset('grok.v.01')]          3   \n",
       "\n",
       "                                         all_synsets  \\\n",
       "0  [Synset('locate.v.01'), Synset('situate.v.01')...   \n",
       "1  [Synset('state.v.01'), Synset('allege.v.01'), ...   \n",
       "2  [Synset('set.n.01'), Synset('set.n.02'), Synse...   \n",
       "3  [Synset('model.n.01'), Synset('framework.n.02'...   \n",
       "4  [Synset('grok.v.01'), Synset('collar.v.01'), S...   \n",
       "\n",
       "                                               gloss  \\\n",
       "0  [[-0.072631836, 0.030517578, -0.040348597, 0.0...   \n",
       "1  [[0.10583496, -0.028564453, 0.11706543, 0.1347...   \n",
       "2  [[0.06587728, 0.04269409, 0.084360756, 0.18495...   \n",
       "3  [[0.07076416, -0.10839844, 0.08298111, 0.05541...   \n",
       "4  [[0.10872396, -0.075927734, 0.015950521, 0.103...   \n",
       "\n",
       "                                            examples  \\\n",
       "0  [[0.02551651, 0.0703125, -0.01953125, 0.083292...   \n",
       "1  [[0.024027506, -0.02319336, 0.025390625, 0.027...   \n",
       "2  [[0.026018415, 0.17170061, 0.113442555, 0.0655...   \n",
       "3  [[0.040754046, -0.007585798, 0.050432477, 0.09...   \n",
       "4  [[0.032552082, -0.050130207, 0.06640625, -0.08...   \n",
       "\n",
       "                                          all_lemmas  \\\n",
       "0  [[locate%2:40:00::], [locate%2:42:00::], [loca...   \n",
       "1  [[say%2:32:00::], [say%2:32:01::], [say%2:32:0...   \n",
       "2  [[set%1:14:00::], [set%1:14:02::], [set%1:04:0...   \n",
       "3  [[framework%1:09:00::], [framework%1:07:00::],...   \n",
       "4  [[apprehend%2:31:00::], [apprehend%2:35:00::],...   \n",
       "\n",
       "                                             result  \n",
       "0                 [[locate%2:42:00::], [0.6718176]]  \n",
       "1           [[say%2:32:04::], [0.7423840704450704]]  \n",
       "2  [[set%5:00:00:arranged:00], [0.989714100853444]]  \n",
       "3             [[framework%1:07:00::], [0.89532065]]  \n",
       "4     [[apprehend%2:31:00::], [0.8523975690929537]]  "
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.13969832019198, 9367.0, 23336)"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_lemmas</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[say%2:32:01::]</td>\n",
       "      <td>[[say%2:32:04::], 1.2213865055236965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[man%1:18:00::]</td>\n",
       "      <td>[[man%1:18:05::], 1.8875896333869828]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[peer%2:39:00::]</td>\n",
       "      <td>[[peer%2:39:00::], 1.3647707283496857]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bleary%5:00:00:indistinct:00]</td>\n",
       "      <td>[[bleary%5:00:00:indistinct:00], 0.91221615797...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[companion%1:18:00::]</td>\n",
       "      <td>[[companion%1:18:00::], 1.0065360311805387]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[eye%1:08:00::]</td>\n",
       "      <td>[[eye%1:08:00::], 1.2380499206787665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[surprise%1:12:00::]</td>\n",
       "      <td>[[surprise%2:31:00::], 1.3377663999795915]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[much%3:00:00::]</td>\n",
       "      <td>[[much%4:02:04::], 1.283877014350709]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[ready%5:00:01:available:00]</td>\n",
       "      <td>[[ready%3:00:00::], 1.5897204950626866]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[answer%1:04:00::]</td>\n",
       "      <td>[[answer%2:31:00::], 1.6709365958543396]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[fit%1:26:00::]</td>\n",
       "      <td>[[fit%2:30:01::], 1.8749669453101965]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[buddy%1:18:00::]</td>\n",
       "      <td>[[buddy%1:18:00::], 0.5148319900035858]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[crazy%5:00:00:insane:00]</td>\n",
       "      <td>[[crazy%5:00:00:insane:00], 1.3938020473105976]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[eye%1:08:00::]</td>\n",
       "      <td>[[eye%1:08:00::], 0.9357497857755206]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[drunk%3:00:00::]</td>\n",
       "      <td>[[drunk%5:00:00:excited:00], 1.4285870134897771]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[drink%1:13:04::, drink%1:13:04::]</td>\n",
       "      <td>[[drink%2:34:02::], 1.4349009706627602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[drink%1:13:04::, drink%1:13:04::]</td>\n",
       "      <td>[[drink%2:34:02::], 1.2857788912258639]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[match%2:42:00::]</td>\n",
       "      <td>[[match%2:35:08::], 1.1324591927210679]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[count%1:23:00::]</td>\n",
       "      <td>[[count%2:32:01::], 1.2015999587348727]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[man%1:18:00::, man%1:18:00::]</td>\n",
       "      <td>[[man%1:18:05::], 1.5434011866375485]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[man%1:18:00::, man%1:18:00::]</td>\n",
       "      <td>[[man%1:18:07::], 1.4172515178230298]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[lose%2:40:02::]</td>\n",
       "      <td>[[lose%2:35:00::], 1.1706298653045561]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[clear%3:00:02::]</td>\n",
       "      <td>[[clear%3:00:00::], 1.2786786511248605]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[guy%1:18:00::]</td>\n",
       "      <td>[[guy%1:18:00::], 1.3477262541651727]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[rocker%1:06:01::]</td>\n",
       "      <td>[[rocker%1:18:01::], 0.4962481935460076]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[eye%1:08:00::]</td>\n",
       "      <td>[[eye%1:08:00::], 1.330018759461653]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[think%2:31:12::]</td>\n",
       "      <td>[[think%2:31:09::], 1.453348200657095]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           true_lemmas  \\\n",
       "0                      [say%2:32:01::]   \n",
       "1                      [man%1:18:00::]   \n",
       "2                     [peer%2:39:00::]   \n",
       "3       [bleary%5:00:00:indistinct:00]   \n",
       "4                [companion%1:18:00::]   \n",
       "5                      [eye%1:08:00::]   \n",
       "6                 [surprise%1:12:00::]   \n",
       "7                     [much%3:00:00::]   \n",
       "8         [ready%5:00:01:available:00]   \n",
       "9                   [answer%1:04:00::]   \n",
       "10                     [fit%1:26:00::]   \n",
       "11                   [buddy%1:18:00::]   \n",
       "12           [crazy%5:00:00:insane:00]   \n",
       "13                     [eye%1:08:00::]   \n",
       "14                   [drunk%3:00:00::]   \n",
       "15  [drink%1:13:04::, drink%1:13:04::]   \n",
       "16  [drink%1:13:04::, drink%1:13:04::]   \n",
       "17                   [match%2:42:00::]   \n",
       "18                   [count%1:23:00::]   \n",
       "19      [man%1:18:00::, man%1:18:00::]   \n",
       "20      [man%1:18:00::, man%1:18:00::]   \n",
       "21                    [lose%2:40:02::]   \n",
       "22                   [clear%3:00:02::]   \n",
       "23                     [guy%1:18:00::]   \n",
       "24                  [rocker%1:06:01::]   \n",
       "25                     [eye%1:08:00::]   \n",
       "26                   [think%2:31:12::]   \n",
       "\n",
       "                                               result  \n",
       "0               [[say%2:32:04::], 1.2213865055236965]  \n",
       "1               [[man%1:18:05::], 1.8875896333869828]  \n",
       "2              [[peer%2:39:00::], 1.3647707283496857]  \n",
       "3   [[bleary%5:00:00:indistinct:00], 0.91221615797...  \n",
       "4         [[companion%1:18:00::], 1.0065360311805387]  \n",
       "5               [[eye%1:08:00::], 1.2380499206787665]  \n",
       "6          [[surprise%2:31:00::], 1.3377663999795915]  \n",
       "7               [[much%4:02:04::], 1.283877014350709]  \n",
       "8             [[ready%3:00:00::], 1.5897204950626866]  \n",
       "9            [[answer%2:31:00::], 1.6709365958543396]  \n",
       "10              [[fit%2:30:01::], 1.8749669453101965]  \n",
       "11            [[buddy%1:18:00::], 0.5148319900035858]  \n",
       "12    [[crazy%5:00:00:insane:00], 1.3938020473105976]  \n",
       "13              [[eye%1:08:00::], 0.9357497857755206]  \n",
       "14   [[drunk%5:00:00:excited:00], 1.4285870134897771]  \n",
       "15            [[drink%2:34:02::], 1.4349009706627602]  \n",
       "16            [[drink%2:34:02::], 1.2857788912258639]  \n",
       "17            [[match%2:35:08::], 1.1324591927210679]  \n",
       "18            [[count%2:32:01::], 1.2015999587348727]  \n",
       "19              [[man%1:18:05::], 1.5434011866375485]  \n",
       "20              [[man%1:18:07::], 1.4172515178230298]  \n",
       "21             [[lose%2:35:00::], 1.1706298653045561]  \n",
       "22            [[clear%3:00:00::], 1.2786786511248605]  \n",
       "23              [[guy%1:18:00::], 1.3477262541651727]  \n",
       "24           [[rocker%1:18:01::], 0.4962481935460076]  \n",
       "25               [[eye%1:08:00::], 1.330018759461653]  \n",
       "26             [[think%2:31:09::], 1.453348200657095]  "
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[[\"true_lemmas\",\"result\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34.3520782396088, 562.0, 1636)"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.466403162055336, 900.0, 2024)"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(true_senses, result):\n",
    "    all_lems = []\n",
    "    for s in true_senses:\n",
    "        for l in s.lemmas():\n",
    "            all_lems.append(l.key())\n",
    "    try:\n",
    "        res = result[:-1]\n",
    "        f = []\n",
    "        for r in res:\n",
    "            for k in r.split(\",\"):\n",
    "                if len(k)>0:\n",
    "                    f.append(k)\n",
    "        #res = set([res.lower() for res in res.split(\",\") if len(res)>0])\n",
    "        res = set(f)\n",
    "        all_lems = set(s.lower() for s in all_lems)\n",
    "        if len(all_lems.intersection(res))>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def show_acc(df):\n",
    "    res = df.apply(lambda x: get_acc(x.true_synset, x.result), axis=1).dropna()\n",
    "    corr = res.sum()\n",
    "    all = res.shape[0]\n",
    "\n",
    "    return (corr/all)*100, corr, all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [45:08<00:00, 11.38s/it] \n"
     ]
    }
   ],
   "source": [
    "s2_df = final_wsd_(doc_senseval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.565217391304344, 902.0, 2024)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(s2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [38:29<00:00,  7.70s/it] \n"
     ]
    }
   ],
   "source": [
    "s3_df = final_wsd_(doc_senseval3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33.924205378973106, 555.0, 1636)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(s3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [8:57:48<00:00,  6.45s/it]   \n"
     ]
    }
   ],
   "source": [
    "sc_df = final_wsd_(sample_semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_lemmas</th>\n",
       "      <th>true_synset</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[locate%2:42:00::]</td>\n",
       "      <td>[Synset('situate.v.01')]</td>\n",
       "      <td>[locate%2:40:01::,place%2:40:00::,site%2:40:00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[say%2:32:00::]</td>\n",
       "      <td>[Synset('state.v.01')]</td>\n",
       "      <td>[say%2:32:13::,, 0.6240999814727821]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[set%2:32:00::]</td>\n",
       "      <td>[Synset('specify.v.02')]</td>\n",
       "      <td>[laid%5:00:00:arranged:00,set%5:00:00:arranged...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[framework%1:09:00::]</td>\n",
       "      <td>[Synset('model.n.01')]</td>\n",
       "      <td>[model%1:09:00::,framework%1:09:00::,, 1.0085602]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[apprehend%2:31:00::]</td>\n",
       "      <td>[Synset('grok.v.01')]</td>\n",
       "      <td>[apprehend%2:37:00::,, 0.8178087085639515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23938</th>\n",
       "      <td>[backer%1:18:00::]</td>\n",
       "      <td>[Synset('angel.n.03')]</td>\n",
       "      <td>[angel%1:18:02::,backer%1:18:00::,, 0.45259362]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23939</th>\n",
       "      <td>[person%1:03:00::]</td>\n",
       "      <td>[Synset('person.n.01')]</td>\n",
       "      <td>[Hudson%1:18:00::,, 0.4389045449106892]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23940</th>\n",
       "      <td>[two%1:23:00::]</td>\n",
       "      <td>[Synset('two.n.01')]</td>\n",
       "      <td>[two%1:23:00::,2%1:23:00::,II%1:23:00::,deuce%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23941</th>\n",
       "      <td>[cape%1:17:00::]</td>\n",
       "      <td>[Synset('cape.n.01')]</td>\n",
       "      <td>[cape%1:06:00::,mantle%1:06:00::,, 0.941523092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23942</th>\n",
       "      <td>[name%2:32:03::]</td>\n",
       "      <td>[Synset('name.v.01')]</td>\n",
       "      <td>[name%2:32:02::,identify%2:32:00::,, 0.7072973...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23943 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 true_lemmas               true_synset  \\\n",
       "0         [locate%2:42:00::]  [Synset('situate.v.01')]   \n",
       "1            [say%2:32:00::]    [Synset('state.v.01')]   \n",
       "2            [set%2:32:00::]  [Synset('specify.v.02')]   \n",
       "3      [framework%1:09:00::]    [Synset('model.n.01')]   \n",
       "4      [apprehend%2:31:00::]     [Synset('grok.v.01')]   \n",
       "...                      ...                       ...   \n",
       "23938     [backer%1:18:00::]    [Synset('angel.n.03')]   \n",
       "23939     [person%1:03:00::]   [Synset('person.n.01')]   \n",
       "23940        [two%1:23:00::]      [Synset('two.n.01')]   \n",
       "23941       [cape%1:17:00::]     [Synset('cape.n.01')]   \n",
       "23942       [name%2:32:03::]     [Synset('name.v.01')]   \n",
       "\n",
       "                                                  result  \n",
       "0      [locate%2:40:01::,place%2:40:00::,site%2:40:00...  \n",
       "1                   [say%2:32:13::,, 0.6240999814727821]  \n",
       "2      [laid%5:00:00:arranged:00,set%5:00:00:arranged...  \n",
       "3      [model%1:09:00::,framework%1:09:00::,, 1.0085602]  \n",
       "4             [apprehend%2:37:00::,, 0.8178087085639515]  \n",
       "...                                                  ...  \n",
       "23938    [angel%1:18:02::,backer%1:18:00::,, 0.45259362]  \n",
       "23939            [Hudson%1:18:00::,, 0.4389045449106892]  \n",
       "23940  [two%1:23:00::,2%1:23:00::,II%1:23:00::,deuce%...  \n",
       "23941  [cape%1:06:00::,mantle%1:06:00::,, 0.941523092...  \n",
       "23942  [name%2:32:02::,identify%2:32:00::,, 0.7072973...  \n",
       "\n",
       "[23943 rows x 3 columns]"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_df[[\"true_lemmas\", \"true_synset\", \"result\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.123354915153826, 8300.0, 23631)"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(sc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension, Training a Word2vec model using few samples of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabaw\\AppData\\Local\\Temp\\ipykernel_23188\\1965273610.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  f_semcor = np.random.choice( documents, size=50)\n",
      "C:\\Users\\nabaw\\AppData\\Local\\Temp\\ipykernel_23188\\1965273610.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  f_senseval2 = np.random.choice( doc_senseval2, size=50)\n",
      "C:\\Users\\nabaw\\AppData\\Local\\Temp\\ipykernel_23188\\1965273610.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  f_senseval3 = np.random.choice( doc_senseval3, size=50)\n"
     ]
    }
   ],
   "source": [
    "#fix seed to produce same result everytime\n",
    "np.random.seed(2022)\n",
    "#choosing 50 samples of each dataset\n",
    "f_semcor = np.random.choice( documents, size=50)\n",
    "f_senseval2 = np.random.choice( doc_senseval2, size=50)\n",
    "f_senseval3 = np.random.choice( doc_senseval3, size=50)\n",
    "#------------------\n",
    "#making one big dataset\n",
    "altered_dataset = []\n",
    "for s, s2, s3 in zip(f_semcor, f_senseval2, f_senseval3):\n",
    "    altered_dataset.append(s)\n",
    "    altered_dataset.append(s2)\n",
    "    altered_dataset.append(s3)\n",
    "#----------------\n",
    "#storing all the texts for later training our model\n",
    "sentences_ = []\n",
    "\n",
    "for s in altered_dataset:\n",
    "    text = \" \".join([x.text for x in s])\n",
    "    text = utils.simple_preprocess(text)\n",
    "    #appending texts in every snetence\n",
    "    sentences_.append(text)\n",
    "    for word in text:\n",
    "        syns = wn.synsets(word)\n",
    "        for syn in syns:\n",
    "            #---------\n",
    "            definition = utils.simple_preprocess(syn.definition())\n",
    "            if len(definition)>0:\n",
    "                #appending all the defintions\n",
    "                sentences_.append(definition)\n",
    "            #------------------\n",
    "            examples = \" \".join(syn.examples())\n",
    "            if len(examples)>0:\n",
    "                examples = utils.simple_preprocess(examples)\n",
    "                #appending all the examples\n",
    "                sentences_.append(examples)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['houghton',\n",
       " 'said',\n",
       " 'had',\n",
       " 'been',\n",
       " 'set',\n",
       " 'for',\n",
       " 'canvass',\n",
       " 'of',\n",
       " 'all',\n",
       " 'homes',\n",
       " 'in',\n",
       " 'the',\n",
       " 'subdivision',\n",
       " 'which',\n",
       " 'is',\n",
       " 'located',\n",
       " 'northeast',\n",
       " 'of',\n",
       " 'dequindre',\n",
       " 'and',\n",
       " 'mile',\n",
       " 'road',\n",
       " 'east']"
      ]
     },
     "execution_count": 857,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library gensim (contains word2vec implementation)\n",
    "import gensim\n",
    "\n",
    "# ignore some warnings (probably caused by gensim version)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "w2v_model_extnesion = gensim.models.Word2Vec(min_count=3,\n",
    "                                   window=3,\n",
    "                                   vector_size=300,  # this parameter is called 'vector_size' in more recent versions of gensim\n",
    "                                   workers=cores,\n",
    "                                   sg=1\n",
    "                                   )\n",
    "\n",
    "# defining the vocabulary based on our data\n",
    "w2v_model_extnesion.build_vocab(sentences_, progress_per=10000)\n",
    "\n",
    "# training\n",
    "w2v_model_extnesion.train(sentences_, total_examples=w2v_model_extnesion.corpus_count, epochs=10, report_delay=1)\n",
    "w2v_model_extnesion.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_extnesion.wv[\"road\",\"man\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"road\" in w2v_model_extnesion.wv.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as there are few changes in the word2vec model we trained and the pre-trained one, we need to rewrite the code a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from operator import itemgetter\n",
    "from gensim import matutils\n",
    "\n",
    "def get_acc(true_senses, result):\n",
    "    all_lems = []\n",
    "    for s in true_senses:\n",
    "        for l in s.lemmas():\n",
    "            all_lems.append(l.key())\n",
    "    try:\n",
    "        res = result[:-1][0]\n",
    "        f = []\n",
    "        for r in res:\n",
    "            for k in r.split(\",\"):\n",
    "                if len(k)>0:\n",
    "                    f.append(k)\n",
    "        #res = set([res.lower() for res in res.split(\",\") if len(res)>0])\n",
    "        res = set(f)\n",
    "        all_lems = set(s.lower() for s in all_lems)\n",
    "        if len(all_lems.intersection(res))>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def show_acc(df):\n",
    "    res = df.apply(lambda x: get_acc(x.true_synset, x.result), axis=1).dropna()\n",
    "    corr = res.sum()\n",
    "    all = res.shape[0]\n",
    "\n",
    "    return (corr/all)*100, corr, all\n",
    "\n",
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity = np.dot(matutils.unitvec(vec1.reshape(1,-1)), matutils.unitvec(vec2))\n",
    "    return cosine_similarity\n",
    "\n",
    "def lemma_mixer(lem, lex):\n",
    "    lem = str(lem).lower()\n",
    "    lex = str(lex).lower().split(\";\")\n",
    "    for i in range(len(lex)):\n",
    "        lex[i]= lem+\"%\"+lex[i]\n",
    "    return \",\".join(lex)\n",
    "\n",
    "def gloss_vec(x, model):\n",
    "    defs_ = []\n",
    "    for syn in x:\n",
    "        sc = utils.simple_preprocess(syn.definition())\n",
    "        sc = [token for token in sc if token in model.wv.index_to_key and token not in stopwords.words('english')]\n",
    "        try:\n",
    "            sc = np.mean(model.wv[sc], axis=0)\n",
    "        except:\n",
    "            sc = np.zeros((1,300), dtype=float)\n",
    "        defs_.append(sc)\n",
    "    return defs_\n",
    "    \n",
    "def example_vec(x, model):\n",
    "    defs_ = []\n",
    "    for syn in x:\n",
    "        sc = utils.simple_preprocess(\".\".join(syn.examples()))\n",
    "        sc = [token for token in sc if token in model.wv.index_to_key and token not in stopwords.words('english')]\n",
    "        try:\n",
    "            sc = np.mean(model.wv[sc], axis=0)\n",
    "        except:\n",
    "            sc = np.zeros((1,300), dtype=float)\n",
    "        defs_.append(sc)\n",
    "    return defs_\n",
    "\n",
    "def lemma_finder(wn_syn, lem_n):\n",
    "    lem_n = lem_n[0].split(\"%\")[0]+\"%\"\n",
    "    lem_syns = []\n",
    "    for lem in wn_syn:\n",
    "        lems = []\n",
    "        for lem in lem.lemmas():\n",
    "            if str(lem.key()).startswith(lem_n):\n",
    "                lems.append(lem.key())\n",
    "        if len(lems)>0:\n",
    "            lem_syns.append(lems)\n",
    "    return lem_syns\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "def finder_l(x, w):\n",
    "    xx = set(wx for wx in x.lower().split(\",\") if len(wx)>0)\n",
    "    w = [y[0] for y in w]\n",
    "    ww = set(x.lower() for x in w if len(x)>0)\n",
    "    #looking for lexemes with lemma key.\n",
    "    res = bool(xx & ww)\n",
    "    if res:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def mapp_finder(df, w):\n",
    "    return df[df[\"wordlemma\"].apply(lambda x: finder_l(x, w))]\n",
    "\n",
    "def lex_creator(key, lemkey):\n",
    "    keyz = []\n",
    "    lemkey = [y for y in lemkey.split(\",\") if len(y)>0]\n",
    "    for x in lemkey:\n",
    "        if len(x)>0:\n",
    "            keyz.append(x.split(\"%\")[0]+\"-\"+key)\n",
    "    return keyz\n",
    "\n",
    "\n",
    "def disambiguator(word, true_lemmas, lemmas, gloss, examples,model,  alpha, beta, lambda_):\n",
    "    if word not in context:\n",
    "        return None\n",
    "    #if no disambiguated words exist\n",
    "    if len(dis_vec)==0:\n",
    "        contxt_vec = model.wv[context]\n",
    "    #if we had disamibuated words earlier \n",
    "    else:                      \n",
    "        alread_dis = np.concatenate(dis_vec, axis=0)\n",
    "        contxt_vec = np.concatenate((alread_dis,model.wv[context]), axis=0)\n",
    "\n",
    "    context_mean_vec = np.mean(contxt_vec, axis=0)\n",
    "    #counter to catch the first word\n",
    "    i = 0\n",
    "    #highest cosine similarity\n",
    "    cos_high = 0\n",
    "    #best syns id\n",
    "    syns_id = 0\n",
    "    #syns of the chosen word\n",
    "    syns_word_selected = 0\n",
    "    #synset wordnet\n",
    "    synset_wd = 0\n",
    "    s = 0\n",
    "    #lem_n = true_lemmas[0].split(\"%\")[0]\n",
    "    keyz = mapp_finder(mapp_df, lemmas)\n",
    "    # keyz[\"lex_keyz\"] = keyz.apply(lambda x: lex_creator(x.wkey, x.wordlemma), axis=1)\n",
    "    #lemma key ---------------\n",
    "    for lem_syn in lemmas:\n",
    "        gloss_vec = gloss[s]\n",
    "        examples_vec = examples[s]\n",
    "        if (len(gloss_vec)>0 and alpha != 0):\n",
    "            score_sw = similarity_cosine(gloss_vec.flatten(), context_mean_vec.flatten()) #syns_embedding\n",
    "        else:\n",
    "            score_sw = 0\n",
    "        # keyz = mapp_finder(mapp_df, lem_syn)\n",
    "        if (len(examples_vec)>0 and lambda_ != 0):\n",
    "            score_example_s = similarity_cosine(examples_vec.flatten(), context_mean_vec.flatten()) \n",
    "        else:\n",
    "            score_example_s = 0\n",
    "\n",
    "        for syn in lem_syn:\n",
    "            #----------------\n",
    "            #exact map index\n",
    "            \n",
    "            map_idx = keyz[keyz.iloc[:,1].apply(lambda x: True if syn in x.lower().split(\",\") else False)]\n",
    "            #finding synset embedding\n",
    "            try:\n",
    "                map_lemmakey = map_idx.iloc[:,0].tolist()[0]\n",
    "            except:\n",
    "                continue\n",
    "            syns_embedding = synsets_emb[map_lemmakey]\n",
    "            #--------------------\n",
    "            if beta != 0:\n",
    "                #finding lexeme------------\n",
    "                lem_n = map_idx.iloc[:,1].tolist()[0].split(\"%\")[0]\n",
    "                lex_key = lem_n+\"-\"+map_lemmakey\n",
    "                try:\n",
    "                    lexeme_embedding = lexemes_emb[lex_key]\n",
    "                    score_lsw = similarity_cosine(lexeme_embedding.flatten(), context_mean_vec.flatten())\n",
    "                except:\n",
    "                    #continue\n",
    "                    score_lsw = 0#similarity_cosine(np.zeros((1,300), dtype=float).flatten(), context_mean_vec.flatten())\n",
    "            else:\n",
    "                score_lsw = 0\n",
    "            #---------------------------------\n",
    "\n",
    "            #final score\n",
    "            score = alpha*score_sw + beta*score_lsw + lambda_*score_example_s\n",
    "            if i == 0:\n",
    "                cos_high = score\n",
    "                syns_id = lem_syn#map_idx.iloc[:,1].tolist()[0]\n",
    "                syns_word_selected = syns_embedding#syns_embedding\n",
    "                #synset_wd = wn_syn\n",
    "                i += 1\n",
    "            if score>cos_high:\n",
    "                cos_high = score\n",
    "                syns_id = lem_syn#map_idx.iloc[:,1].tolist()[0]\n",
    "                syns_word_selected = syns_embedding#syns_embedding    \n",
    "                #synset_wd = wn_syn       \n",
    "        s += 1     \n",
    "\n",
    "    #if i was able to disambiguate the word add it so next time only the synset embedding is used\n",
    "    if i>0:\n",
    "        context.remove(word)\n",
    "        dis_vec.append(np.array(syns_word_selected, dtype=float).reshape(1,-1))\n",
    "    return [syns_id, cos_high]\n",
    "\n",
    "def final_wsd_wv(docs, model=word2vec, alpha=0.5, beta=0.1, lambda_=0.9):\n",
    "    full = []\n",
    "    for doc in tqdm(docs):\n",
    "    #one sentence\n",
    "        one_sent = utils.simple_preprocess(\" \".join([x.text for x in doc]))\n",
    "    #a list to store word, lemmakey, number of senses, sense key, syns\n",
    "        _sent = []\n",
    "    #every word in the sentence\n",
    "        for wrd in one_sent:\n",
    "        #make lemma key\n",
    "            lemk = [lemma_mixer(x.lemma, x.lexsn) for x in doc if ((\"\".join(utils.simple_preprocess(x.text)) == wrd)and((x.lexsn != None) and (x.lemma != None)))]\n",
    "        # if there is lexsn and lemma\n",
    "        #if len(lemk)>0:\n",
    "            try:\n",
    "            #making the lemma key, true one given by sentence.\n",
    "                lemk = \",\".join(lemk).split(\",\")\n",
    "                syns_t = [wn.lemma_from_key(l).synset() for l in lemk]\n",
    "            except:\n",
    "                continue\n",
    "            #lemk = None\n",
    "            #using wordnet, find all the synsets for the word\n",
    "            syns = wn.synsets(wrd)\n",
    "\n",
    "            _sent.append([wrd, lemk,syns_t, len(syns),  syns])\n",
    "        if len(_sent)==0:\n",
    "            continue\n",
    "        global dis_vec\n",
    "        dis_vec = []\n",
    "        #sorting\n",
    "        _sent = sorted(_sent, key=itemgetter(3))\n",
    "        global context\n",
    "        context = [token[0] for token in _sent if token[0] in model.wv.index_to_key and token[0] not in stopwords.words('english')]\n",
    "        _sent = pd.DataFrame(_sent, columns=[\"word\", \"true_lemmas\",\"true_synset\", \"frequency\", \"all_synsets\"])\n",
    "        _sent[\"gloss\"] = _sent[\"all_synsets\"].apply(lambda x: gloss_vec(x, model))\n",
    "        _sent[\"examples\"] = _sent[\"all_synsets\"].apply(lambda x: example_vec(x, model))\n",
    "        _sent[\"all_lemmas\"]=_sent.apply(lambda x: lemma_finder(x.all_synsets, x.true_lemmas), axis=1)\n",
    "        _sent[\"result\"]=_sent.apply(lambda x: disambiguator(x.word, x.true_lemmas, x.all_lemmas, x.gloss, x.examples, model, alpha, beta, lambda_), axis=1)\n",
    "        full.append(_sent)\n",
    "    full = pd.concat(full)\n",
    "\n",
    "    return full.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [16:11<00:00,  6.48s/it]\n"
     ]
    }
   ],
   "source": [
    "#extension\n",
    "altered = final_wsd_wv(altered_dataset, model=w2v_model_extnesion, alpha=1, beta=1, lambda_=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28.621495327102803, 245.0, 856)"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(altered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [16:51<00:00,  6.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31.89252336448598, 273.0, 856)"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extension\n",
    "altered1 = final_wsd_wv(altered_dataset, model=w2v_model_extnesion, alpha=0.5, beta=0.1, lambda_=1.2)\n",
    "show_acc(altered1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [16:10<00:00,  6.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.981308411214954, 308.0, 856)"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extension\n",
    "altered3 = final_wsd_wv(altered_dataset, model=w2v_model_extnesion, alpha=0, beta=0.8, lambda_=0.5)\n",
    "show_acc(altered3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extension\n",
    "altered4 = final_wsd_wv(altered_dataset, model=w2v_model_extnesion, alpha=0.8, beta=0, lambda_=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30.724299065420563, 263.0, 856)"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_acc(altered4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61.06194690265486, 552, 904)"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_cs = find_most_common_sense(altered_dataset)\n",
    "get_acc(alt_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 164.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49.345417925478344, 490, 993)"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lescs = lesk_mc(altered_dataset)\n",
    "evaluate_accuracy(lescs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential code of LESK++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def finder(x, w):\n",
    "    xx = set(wx for wx in x.lower().split(\",\") if len(wx)>0)\n",
    "    ww = set(x.lower() for x in w if len(x)>0)\n",
    "    #looking for lexemes with lemma key.\n",
    "    res = bool(xx & ww)\n",
    "    if res:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def mapp_finder(df, w):\n",
    "    return df[df.iloc[:,-1].apply(lambda x: finder(x, w))]\n",
    "\n",
    "def lemma_mixer(lem, lex):\n",
    "    lem = str(lem).lower()\n",
    "    lex = str(lex).lower().split(\";\")\n",
    "    for i in range(len(lex)):\n",
    "        lex[i]= lem+\"%\"+lex[i]\n",
    "    return \",\".join(lex)\n",
    "\n",
    "\n",
    "from gensim import matutils\n",
    "def similarity_cosine(vec1, vec2):\n",
    "    cosine_similarity = np.dot(matutils.unitvec(vec1), matutils.unitvec(vec2))\n",
    "    return cosine_similarity\n",
    "\n",
    "def wsd_pb(docs):\n",
    "    full_sent = []\n",
    "    for doc in tqdm(docs):\n",
    "        #one sentence\n",
    "        one_sent = utils.simple_preprocess(\" \".join([x.text for x in doc]))\n",
    "        #a list to store word, lemmakey, number of senses, sense key, syns\n",
    "        _sent = []\n",
    "        #every word in the sentence\n",
    "        for wrd in one_sent:\n",
    "            #make lemma key\n",
    "            lemk = [lemma_mixer(x.lemma, x.lexsn) for x in doc if ((\"\".join(utils.simple_preprocess(x.text)) == wrd)and((x.lexsn != None) and (x.lemma != None)))]\n",
    "            # if there is lexsn and lemma\n",
    "            if len(lemk)>0:\n",
    "                #making the lemma key, true one given by sentence.\n",
    "                lemk = \",\".join(lemk)\n",
    "            else:\n",
    "                continue\n",
    "                #lemk = None\n",
    "            #using wordnet, find all the synsets for the word\n",
    "            syns = wn.synsets(wrd)\n",
    "            _sent.append([wrd,lemk, len(syns), syns])\n",
    "        #sorting\n",
    "        _sent = sorted(_sent, key=itemgetter(2))\n",
    "\n",
    "        #vector for disambiguated words\n",
    "        dis_vec = []\n",
    "\n",
    "        context = [token[0] for token in _sent if token[0] in word2vec.index_to_key and token[0] not in stopwords.words('english')]# and token[0] not in stopwords.words('english')\n",
    "        # try:\n",
    "        for ambigus_w in _sent:\n",
    "            #if we don't have the word in the context skip it\n",
    "            if ambigus_w[0] not in context:\n",
    "                continue\n",
    "            #counter to catch the first word\n",
    "            i = 0\n",
    "            #highest cosine similarity\n",
    "            cos_high = 0\n",
    "            #best syns id\n",
    "            syns_id = 0\n",
    "            #syns of the chosen word\n",
    "            syns_word_selected = 0\n",
    "            #synset wordnet\n",
    "            synset_wd = 0\n",
    "            lem_n = ambigus_w[1].split(\"%\")[0]\n",
    "            for wn_syn in ambigus_w[3]:\n",
    "                #gloss-------------------------------------------\n",
    "                gloss = [w for w in wn_syn.definition().split() if w in word2vec.index_to_key and w not in stopwords.words('english')]\n",
    "                if len(gloss)>0:\n",
    "                    gloss_vec = np.mean(word2vec[gloss], axis=0)\n",
    "                else:\n",
    "                    gloss_vec = np.zeros((1,300), dtype=float)\n",
    "                #--------------------------------------------------\n",
    "\n",
    "                #getting all the lemas-----------------\n",
    "                lems = []\n",
    "                for lem in wn_syn.lemmas():\n",
    "                    if str(lem.key()).startswith(lem_n):\n",
    "                        lems.append(lem.key())\n",
    "                #----------------\n",
    "                #if there is no lemma go to the next synset---\n",
    "                if len(lems)==0:\n",
    "                    continue\n",
    "                #-------------------\n",
    "                #only choosing the lemmaskey existing this synset\n",
    "                keyz = mapp_finder(mapp_df, lems)\n",
    "                #-------------------\n",
    "\n",
    "                #if no disambiguated words exist\n",
    "                if len(dis_vec)==0:\n",
    "                    contxt_vec = word2vec[context]\n",
    "                #if we had disamibuated words earlier \n",
    "                else:                      \n",
    "                    alread_dis = np.concatenate(dis_vec, axis=0)\n",
    "                    contxt_vec = np.concatenate((alread_dis,word2vec[context]), axis=0)\n",
    "\n",
    "                context_mean_vec = np.mean(contxt_vec, axis=0)\n",
    "                score_sw = similarity_cosine(gloss_vec.flatten(), context_mean_vec.flatten()) #syns_embedding\n",
    "\n",
    "                #lemma key ---------------\n",
    "                for syn in lems:\n",
    "                    #----------------\n",
    "                    #exact map index\n",
    "                    map_idx = keyz[keyz.iloc[:,1].apply(lambda x: True if syn in x.lower().split(\",\") else False)]\n",
    "                    #finding synset embedding\n",
    "                    try:\n",
    "                        map_lemmakey = map_idx.iloc[:,0].tolist()[0]\n",
    "                    except:\n",
    "                        continue\n",
    "                    syns_embedding = synsets_emb[map_lemmakey]\n",
    "                    #--------------------\n",
    "\n",
    "                    #finding lexeme------------\n",
    "                    lex_key = lem_n+\"-\"+map_lemmakey\n",
    "                    try:\n",
    "                        lexeme_embedding = lexemes_emb[lex_key]\n",
    "                        score_lsw = similarity_cosine(lexeme_embedding.flatten(), context_mean_vec.flatten())\n",
    "                    except:\n",
    "                        score_lsw = similarity_cosine(np.zeros((1,300), dtype=float).flatten(), context_mean_vec.flatten())\n",
    "                    #---------------------------------\n",
    "\n",
    "                    #final score\n",
    "                    score = score_sw+score_lsw\n",
    "                    if i == 0:\n",
    "                        cos_high = score\n",
    "                        syns_id = map_idx.iloc[:,1].tolist()[0]\n",
    "                        syns_word_selected = syns_embedding\n",
    "                        #synset_wd = wn_syn\n",
    "                        i += 1\n",
    "                    if score>cos_high:\n",
    "                        cos_high = score\n",
    "                        syns_id = map_idx.iloc[:,1].tolist()[0]\n",
    "                        syns_word_selected = syns_embedding    \n",
    "                        #synset_wd = wn_syn            \n",
    "\n",
    "            #if i was able to disambiguate the word add it so next time only the synset embedding is used\n",
    "            if i>0:\n",
    "                context.remove(ambigus_w[0])\n",
    "                dis_vec.append(np.array(syns_word_selected, dtype=float).reshape(1,-1))\n",
    "            try:\n",
    "                full_sent.append([ambigus_w[0], ambigus_w[1],syns_id, cos_high])\n",
    "            except:\n",
    "                pass\n",
    "    return full_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [2:41:15<00:00, 40.66s/it]  \n"
     ]
    }
   ],
   "source": [
    "sa = wsd_pb(doc_senseval2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unintelligible',\n",
       " 'unintelligible%5:00:00:incomprehensible:00',\n",
       " 'unintelligible%3:00:00::,',\n",
       " 0.77900994]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50.23746701846966, 1895, 952)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acc_wsd(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [2:18:03<00:00, 27.61s/it]  \n"
     ]
    }
   ],
   "source": [
    "sb = wsd_pb(doc_senseval3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33.37378640776699, 1648, 550)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acc_wsd(sb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = wsd_pb(sample_semcor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_acc_wsd(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_wsd(results):\n",
    "    all = 0\n",
    "    match = 0\n",
    "    for w in results:\n",
    "        all += 1\n",
    "        for syns in wn.synsets(w[0]):\n",
    "            lems = []\n",
    "            for l in syns.lemmas():\n",
    "                lems.append(l.key())\n",
    "            try:\n",
    "                res_orig = set(w[1].lower().split(\",\")).intersection(set(lems))\n",
    "                res_pred = set(w[2].lower().split(\",\")).intersection(set(lems))\n",
    "                if bool(res_orig & res_pred):\n",
    "                    match += 1\n",
    "            except:\n",
    "                all -= 1\n",
    "                continue\n",
    "\n",
    "    return (match/all)*100, all, match\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c50cf09ab0329ccd20bb189976f78b644f53f0461f30c902e674e2072027ed74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
